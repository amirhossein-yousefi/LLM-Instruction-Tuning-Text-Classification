# LLM Instruction‚ÄëTuning for Text Classification (LoRA + QLoRA)

> **Train a decoder‚Äëonly LLM to follow *classification* instructions using LoRA adapters and 4‚Äëbit quantization.**  
> Minimal, modular, and fast: one config, two scripts (`train.py`, `predict.py`).

---

## üöÄ TL;DR

This repo instruction‚Äëtunes a decoder‚Äëonly LLM (default: **`meta-llama/Llama-3.2-1B`**) to classify short texts (e.g., arXiv titles/abstracts) **by responding with an exact label string**. It uses:

- **PEFT/LoRA** adapters (tiny trainable layers) instead of full fine‚Äëtuning
- **4‚Äëbit (QLoRA‚Äëstyle)** quantization for memory‚Äëefficient training/inference
- **TRL‚Äôs `SFTTrainer`** for supervised instruction tuning
- Simple **prompt templates** that turn classification into a generative task

Out of the box, it ships with an **arXiv‚Äëstyle demo** (5 labels) and example results (~94% accuracy).

---

## ‚ú® What‚Äôs inside

```
LLM-Instruction-Tuning-Text-Classification/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ train.py           # one‚Äëcommand training with TRL + LoRA
‚îÇ   ‚îî‚îÄ‚îÄ predict.py         # attach LoRA adapters & run inference/eval
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # dataclass: all tunables live here (TrainConfig)
‚îÇ   ‚îú‚îÄ‚îÄ data.py            # CSV I/O, prompt builders, seeding, HF dataset adapter
‚îÇ   ‚îú‚îÄ‚îÄ model.py           # 4‚Äëbit (bitsandbytes) + tokenizer + LoRA configs
‚îÇ   ‚îú‚îÄ‚îÄ training.py        # wrapper that builds TRL SFTTrainer
‚îÇ   ‚îú‚îÄ‚îÄ infer.py           # generation pipeline & label decoding
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py         # accuracy/F1, classification report, confusion matrix
‚îÇ   ‚îú‚îÄ‚îÄ prepare_all_arxiver_data.py
‚îÇ   ‚îî‚îÄ‚îÄ prepare_top_5_arxiver_data.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ results.txt            # sample evaluation output
```

---

## üß† Why instruction‚Äëtune a classifier?

Instead of training a bespoke encoder classifier, we **teach a general‚Äëpurpose LLM to follow a classification instruction**. This brings three benefits:

1. **Flexibility:** add/remove labels or extend the schema without changing the head.
2. **Transfer:** one prompting style scales across domains/tasks.
3. **Traceability:** prompts & responses are plain text‚Äîeasy to debug and audit.

---

## üß© Prompt format

The project formulates classification as short **instruction ‚Üí answer** exchanges.

**Training prompts** (supervised):  

```text
Classify the text into {labels_str} and return the answer as the exact text label.
text: {text}
label: {gold_label}
```

**Inference prompts** (generation):  

```text
Classify the text into {labels_str} and return the answer as the exact text label.
text: {text}
label:
```

> The model is evaluated by string‚Äëmatching its generated label to the ground truth.

---

## üì¶ Data format

By default, the repo expects three CSVs inside a folder (see `TrainConfig.base_path`):

- `train.csv`
- `validation.csv`
- `test.csv`

Each CSV should include:
- **`label_name`** (or customize via `--label_column`)
- One or more text fields, default: **`title`**, **`abstract`** (customize via `--text_fields`)

**Minimal example (`train.csv`):**

```csv
title,abstract,label_name
"Neural topic models with...", "We propose...", "cs.CL"
"Vision transformer for...", "We revisit...", "cs.CV"
```

If a text column is missing or blank, it‚Äôs skipped automatically. Text fields are concatenated into a single sentence with proper punctuation.

> Utilities such as `src/prepare_top_5_arxiver_data.py` can help produce demo CSVs. Feel free to adapt them to your own corpus.

---

## ‚öôÔ∏è Configuration (everything in one place)

All defaults live in `src/config.py` (`TrainConfig`). Highlights:

- **Base model:** `meta-llama/Llama-3.2-1B`
- **Labels (demo):** `['cs.CL', 'cs.CV', 'cs.LG', 'hep-ph', 'quant-ph']`
- **LoRA:** `r=2`, `alpha=2`, `dropout=0.0` (easy to scale up)
- **Quantization:** 4‚Äëbit NF4 + bfloat16 compute (QLoRA‚Äëstyle)
- **Sequence length:** 512
- **Trainer:** epochs, batch sizes, eval/save/logging strategies, seed
- **Generation:** deterministic by default (`temperature=0.0`, few tokens)

You can override any config at the CLI (see below).

---

## üõ†Ô∏è Setup

> Python ‚â• 3.10 recommended.

```bash
# 1) Create & activate a virtual env
python -m venv .venv
# Linux/macOS
source .venv/bin/activate
# Windows (PowerShell)
# .venv\Scripts\Activate.ps1

# 2) Install deps
pip install --upgrade pip
pip install -r requirements.txt

# 3) (If your base HF model requires access) set a token:
# Linux/macOS
export HF_TOKEN=YOUR_HF_ACCESS_TOKEN
# Windows (PowerShell)
# $env:HF_TOKEN="YOUR_HF_ACCESS_TOKEN"
```

> **GPU note:** The project expects an NVIDIA CUDA GPU for 4‚Äëbit (bitsandbytes) training and inference. CPU‚Äëonly runs are not supported by the provided scripts.

---

---

## ‚ö° Use `uv` for dependency management (recommended)

[`uv`](https://docs.astral.sh/uv/) is a fast, modern Python package & project manager from Astral (creators of Ruff). It is a drop‚Äëin replacement for most `pip` workflows, with first‚Äëclass virtualenvs, lockfiles, and reproducible installs.

> This repository currently ships a standard **`requirements.txt`** (see project root). You can continue using `pip`, **or** follow these `uv` steps for a faster and more reproducible setup.

### 1) Install `uv`

**macOS / Linux**

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**Windows (PowerShell)**

```powershell
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

> Alternatives: `brew install uv` (macOS), `winget install --id=astral-sh.uv -e` (Windows), or `pipx install uv`.

### 2) Create the virtual environment

`uv` defaults to a project‚Äëlocal `.venv` and will automatically discover and use it.

```bash
uv venv                 # creates .venv (optionally: uv venv --python 3.11)
# optional activation (uv will find .venv even if you skip this)
source .venv/bin/activate        # macOS/Linux
# or on Windows PowerShell
# .\.venv\Scripts\Activate.ps1
```

### 3) Install dependencies from `requirements.txt`

Two equivalent ways, depending on how strict you want to be:

- **Exact, reproducible environment** (removes anything not listed):  
  ```bash
  uv pip sync requirements.txt
  ```
- **Flexible install** (keeps extra packages if already present):  
  ```bash
  uv pip install -r requirements.txt
  ```

> If you later add a package and want it pinned into `requirements.txt`, see **Step 4**.

### 4) (Optional) Lock or upgrade dependencies

If you maintain a `requirements.in` or a `pyproject.toml`, use `uv` to **compile** a fully pinned `requirements.txt`:

```bash
# from a pyproject.toml
uv pip compile pyproject.toml -o requirements.txt

# from a requirements.in
uv pip compile requirements.in -o requirements.txt

# upgrade a single package during compile
uv pip compile - -o requirements.txt --upgrade-package ruff <<< "ruff"
```

Re‚Äëapply the lock to your environment at any time with:
```bash
uv pip sync requirements.txt
```

### 5) (Optional) Migrate to `pyproject.toml` + `uv.lock`

For first‚Äëclass, cross‚Äëplatform locking you can move to a `uv` project with a `pyproject.toml` + `uv.lock`:

```bash
# initialize a minimal project in the current directory
uv init --bare

# import everything from your existing requirements.txt
uv add -r requirements.txt

# (uv will resolve + write uv.lock and sync .venv automatically)
# manually refresh the lockfile later if needed
uv lock

# install/update the environment from the lockfile
uv sync
```

Commit **`pyproject.toml`** and **`uv.lock`** to version control.  
If you still need a `requirements.txt` for external tooling, export it from the lockfile:

```bash
# export exact, pinned requirements
uv export --format requirements-txt --no-hashes > requirements.txt
```

### 6) CI example (GitHub Actions)

```yaml
# .github/workflows/ci.yml
name: ci
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh
      - name: Set up venv + install deps
        run: |
          uv venv --python 3.11
          uv pip sync requirements.txt
      - name: Run tests / scripts
        run: |
          source .venv/bin/activate
          # python -m pytest
          # or: python scripts/train.py ...
```

> **Tip:** `uv` can download a specific Python if missing (e.g., `uv venv --python 3.11`). It will still work fine with a system Python if one is available.

---
## üèÉ‚Äç‚ôÄÔ∏è Quickstart: Train

The simplest run trains LoRA adapters on your CSVs and saves them to an output folder:

```bash
python scripts/train.py   --base_path dataset   --train_file train.csv   --val_file validation.csv   --test_file test.csv   --label_column label_name   --text_fields title abstract   --base_model_name meta-llama/Llama-3.2-1B   --output_dir llama-3.2-1b-arxiver-lora
```

What happens under the hood:

- Loads CSVs ‚Üí builds *instruction* prompts for train/val/test
- Loads the base model in **4‚Äëbit** with **bitsandbytes**
- Wraps the model with **LoRA** adapters (PEFT)
- Runs **TRL‚Äôs `SFTTrainer`** for one or more epochs
- Saves the LoRA weights + tokenizer to `--output_dir`

---

## üîÆ Inference & evaluation

Attach the saved LoRA adapters to the base model and predict:

```bash
python scripts/predict.py   --base_path dataset   --test_file test.csv   --base_model_name meta-llama/Llama-3.2-1B   --output_dir llama-3.2-1b-arxiver-lora   --save_csv predictions.csv
```

This script:

- Recreates the **inference prompt** for each test row
- Generates the label deterministically (`temperature=0.0` by default)
- Computes **accuracy**, **micro/macro F1**, a **classification report**, and a **confusion matrix**
- Optionally writes `predictions.csv` (prompt, gold, pred)

---

## üìà Reference results (demo)

The included `results.txt` (arXiv‚Äëstyle 5‚Äëlabel demo) reports:

- **Accuracy:** 0.938  
- **F1 (micro):** 0.938  
- **F1 (macro):** 0.950

Per‚Äëclass performance and a confusion matrix are also provided in that file. Your mileage will vary with different seeds, tokenization, and label sets.

---

## üß™ Reproducibility

- Global seeding via `seed_everything()`
- Deterministic generation defaults for evaluation
- All hyperparameters centralized in `TrainConfig` for easy experiment tracking

> Tip: Log to **TensorBoard** by keeping `report_to="tensorboard"` and launching:
>
> ```bash
> tensorboard --logdir .
> ```

---

## üßØ Troubleshooting

- **`CUDA GPU is required`** ‚Äî The scripts guard against CPU runs because 4‚Äëbit quantization depends on CUDA (bitsandbytes). Make sure NVIDIA drivers + CUDA toolkit are properly installed for your environment.
- **`bitsandbytes` import errors** ‚Äî Verify the installed `bitsandbytes` version and CUDA compatibility; reinstall if needed. Some Windows setups may require WSL2 for a smoother experience.
- **Model access** ‚Äî For gated models (e.g., Llama family), ensure you‚Äôve accepted the license on Hugging Face and set `HF_TOKEN` in your environment.

---

## üîß Adapting to your dataset

- Replace/extend **labels** with `--labels` or edit `TrainConfig.labels`
- Point to your **label column** with `--label_column`
- Provide one or more **text fields** with `--text_fields title abstract body ...`
- Consider increasing **LoRA rank** (`lora_r`) and **alpha** for tougher tasks
- Increase **context length** (`max_seq_length`) if your texts are long

---

## üó∫Ô∏è Roadmap ideas

- Add support for *multi‚Äëlabel* classification (comma‚Äëseparated labels)
- Add few‚Äëshot exemplars to the prompt template
- Provide ready‚Äëmade dataset builders for common corpora
- Export to merged full‚Äëprecision checkpoint for deployment

---

## üìö References (tech used here)

- **LoRA adapters (PEFT)** ‚Äî Low‚Äërank adapters for parameter‚Äëefficient fine‚Äëtuning  
- **TRL SFTTrainer** ‚Äî Supervised fine‚Äëtuning for decoder‚Äëonly LLMs  
- **4‚Äëbit quantization (NF4)** ‚Äî Memory‚Äëefficient training/inference with bitsandbytes  
- **Llama 3.2 models** ‚Äî 1B/3B multilingual text‚Äëonly models; some variants are gated

---

## ü§ù Acknowledgements

Thanks to the open‚Äësource communities behind **Transformers**, **TRL**, **PEFT**, **bitsandbytes**, and the **Llama** model family. This project stands on their shoulders.

---

## üìÑ License

No explicit license file is provided in this repository at the time of writing. If you plan to use or redistribute the code, please add an appropriate license file and abide by any upstream model licenses (e.g., Llama).

---

## üôå Contributing

Issues and PRs that improve docs, add datasets, or extend training recipes are very welcome!

