#!/usr/bin/env python3

"""
Create a real-time SageMaker endpoint using Hugging Face PyTorch Inference DLC
and the LoRA adapter artifacts produced by the training job.
"""
import argparse
import json
import os
import sys

import boto3
import sagemaker
from sagemaker.huggingface import HuggingFaceModel
from botocore.exceptions import ClientError


def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--model_artifact_s3", default=None,
                   help="s3://... path to model.tar.gz from training. If omitted, --training_job_name is used to resolve it.")
    p.add_argument("--training_job_name", default=None,
                   help="Optional: name of the training job to resolve the model artifact.")
    p.add_argument("--endpoint_name", default=None, help="Name for the endpoint (autogenerated if omitted).")
    p.add_argument("--instance_type", default="ml.g5.2xlarge")
    p.add_argument("--initial_instance_count", type=int, default=1)
    p.add_argument("--region", default=None)
    p.add_argument("--role_arn", default=None)
    # DLC image - by default we use the current GPU inference DLC as of 2025-09 (transformers 4.51.3, py312, cu124)
    p.add_argument("--image_uri", default=None,
                   help="Explicit DLC image URI (recommended). If omitted, SDK will try to resolve based on versions.")
    # Base model + labels for inference
    p.add_argument("--base_model_id", default="meta-llama/Llama-3.2-1B")
    p.add_argument("--default_labels_json", default=None, help='Optional JSON list of label strings to set as DEFAULT_LABELS at the endpoint.')
    p.add_argument("--hf_token", default=None, help="HF token if the base model is gated.")
    # Source code (inference.py + requirements.txt)
    p.add_argument("--source_dir", default="sagemaker/inference", help="Folder containing inference.py and requirements.txt")
    p.add_argument("--entry_point", default="inference.py")
    # Generation defaults
    p.add_argument("--max_new_tokens", type=int, default=8)
    p.add_argument("--temperature", type=float, default=0.0)
    p.add_argument("--num_beams", type=int, default=1)
    p.add_argument("--use_4bit", type=str, default="true")
    return p.parse_args()


def resolve_model_artifact(sagemaker_client, training_job_name: str) -> str:
    try:
        resp = sagemaker_client.describe_training_job(TrainingJobName=training_job_name)
        return resp["ModelArtifacts"]["S3ModelArtifacts"]
    except ClientError as e:
        raise RuntimeError(f"Could not resolve model artifact from training job '{training_job_name}': {e}")


def main():
    args = parse_args()

    boto_sess = boto3.session.Session(region_name=args.region) if args.region else boto3.session.Session()
    region = boto_sess.region_name
    sm_sess = sagemaker.Session(boto_session=boto_sess)
    sm_client = boto_sess.client("sagemaker")

    if args.role_arn:
        role = args.role_arn
    else:
        try:
            role = sagemaker.get_execution_role()
        except Exception:
            raise RuntimeError("role_arn not provided and get_execution_role() failed. Provide --role_arn explicitly.")

    # Resolve model artifact if required
    model_data = args.model_artifact_s3
    if not model_data:
        if not args.training_job_name:
            raise ValueError("Provide --model_artifact_s3 or --training_job_name")
        model_data = resolve_model_artifact(sm_client, args.training_job_name)

    print(f"[INFO] Using model artifact: {model_data}")

    # Select image
    image_uri = args.image_uri
    if not image_uri:
        # Fall back to a current GPU inference DLC from the public docs (adjust region/account if required)
        image_uri = f"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:2.6.0-transformers4.51.3-gpu-py312-cu124-ubuntu22.04"
        print(f"[INFO] Using default GPU inference DLC: {image_uri}")

    # Environment for the container
    env = {
        "BASE_MODEL_ID": args.base_model_id,
        "USE_4BIT": str(args.use_4bit).lower(),
        "MAX_NEW_TOKENS": str(args.max_new_tokens),
        "TEMPERATURE": str(args.temperature),
        "NUM_BEAMS": str(args.num_beams),
    }
    if args.default_labels_json:
        env["DEFAULT_LABELS"] = args.default_labels_json
    if args.hf_token:
        env["HF_TOKEN"] = args.hf_token

    # Create model
    hf_model = HuggingFaceModel(
        role=role,
        model_data=model_data,
        image_uri=image_uri,
        entry_point=args.entry_point,
        source_dir=args.source_dir,
        env=env,
        sagemaker_session=sm_sess,
    )

    endpoint_name = args.endpoint_name or ("lora-tc-ep-" + os.urandom(3).hex())
    print(f"[INFO] Creating endpoint: {endpoint_name} on {args.instance_type} x {args.initial_instance_count}")

    predictor = hf_model.deploy(
        initial_instance_count=args.initial_instance_count,
        instance_type=args.instance_type,
        endpoint_name=endpoint_name,
    )
    print(f"[OK] Endpoint ready: {endpoint_name}")
    print(f"[HINT] Invoke example:\npython sagemaker/invoke_sm.py --endpoint_name {endpoint_name} "
          f"--text 'Quantum entanglement in photonics' --labels '[\"cs.CL\",\"cs.CV\",\"cs.LG\",\"hep-ph\",\"quant-ph\"]'")

if __name__ == "__main__":
    main()
